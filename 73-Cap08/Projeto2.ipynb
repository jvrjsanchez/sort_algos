{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSHlAbqzDFDq"
      },
      "source": [
        "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
        "# <font color='blue'>Data Science Academy</font>\n",
        "## <font color='blue'>IA Generativa e LLMs Para Processamento de Linguagem Natural</font>\n",
        "## <font color='blue'>Projeto 2</font>\n",
        "## <font color='blue'>Fine-Tuning Eficiente de LLMs com LoRA Para Análise de Sentimentos em Texto</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFy-uAhz1OOh"
      },
      "source": [
        "## Instalando e Carregando Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9kKetyp1OOi"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8EUCYFk7W5W"
      },
      "source": [
        "A descrição de cada um dos pacotes abaixo está disponível no Capítulo 7 do Curso.\n",
        "\n",
        "https://www.datascienceacademy.com.br/course/ia-generativa-e-llms-para-processamento-de-linguagem-natural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yBspMX-1OOi"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==1.4.0 peft==0.14.0 bitsandbytes==0.45.3 transformers==4.49.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJOiFhsI1OOi"
      },
      "outputs": [],
      "source": [
        "!pip install -q trl==0.15.2 gradio==5.18.0 protobuf==5.29.3 scipy==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCscFWU11OOi"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentencepiece==0.2.0 tokenizers==0.21.0 datasets==3.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRozZWGN1OOi"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          HfArgumentParser,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCjC_MHpAKQP"
      },
      "outputs": [],
      "source": [
        "# Define o nível de log para CRITICAL\n",
        "logging.set_verbosity(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmFTqTOG61YC"
      },
      "outputs": [],
      "source": [
        "# Verifica o modelo da GPU\n",
        "if torch.cuda.is_available():\n",
        "    print('Número de GPUs:', torch.cuda.device_count())\n",
        "    print('Modelo GPU:', torch.cuda.get_device_name(0))\n",
        "    print('Total Memória [GB] da GPU:',torch.cuda.get_device_properties(0).total_memory / 1e9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw4uSDl76z-N"
      },
      "outputs": [],
      "source": [
        "# Reset da memória da GPU\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZArexSp1OOj"
      },
      "outputs": [],
      "source": [
        "# Define o nome do dataset\n",
        "nome_dataset = \"dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFjD_NF8n8kt"
      },
      "outputs": [],
      "source": [
        "# Carrega os dados\n",
        "dataset_carregado = load_dataset('csv', data_files = nome_dataset, delimiter = ',')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados Carregados no Formato de Dicionário\n",
        "dataset_carregado"
      ],
      "metadata": {
        "id": "fcMKEnkT9UZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/NousResearch/Llama-2-7b-chat-hf"
      ],
      "metadata": {
        "id": "60qumlGz8YI3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe_vsFJs1XPG"
      },
      "outputs": [],
      "source": [
        "# Nome do repositório do LLM pré-treinado\n",
        "repositorio_hf = \"NousResearch/Llama-2-7b-chat-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbPqgPsv1OOj"
      },
      "outputs": [],
      "source": [
        "# Nome do novo modelo\n",
        "modelo_dsa = \"novo_modelo_dsa\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definindo os Parâmetros de Configuração"
      ],
      "metadata": {
        "id": "J7B7VikzQp3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bauIDIDh1OOj"
      },
      "outputs": [],
      "source": [
        "# Parâmetros LoRA\n",
        "lora_r = 32\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os parâmetros acima são do LoRa (Low-Rank Adaptation), a parte base do QLoRA (Quantized Low-Rank Adaptation), uma técnica utilizada para adaptar modelos de linguagem de forma eficiente. Acima dos parâmetros LoRa colocamos os parâmetros de quantização, definindo assim o QLoRa.\n",
        "\n",
        "Vamos descrever cada um dos parâmetros:\n",
        "\n",
        "**lora_r**: Este parâmetro representa o \"rank\" na adaptação de Low-Rank (LoRA). Um valor de 32 significa que a matriz de pesos do modelo original será aproximada por duas matrizes menores cujo produto possui um rank máximo de 32. Essencialmente, isso reduz a complexidade computacional e o número de parâmetros a serem treinados durante a adaptação, mantendo a eficácia do modelo.\n",
        "\n",
        "**lora_alpha**: Este é um fator de escala que é aplicado às atualizações de peso do LoRA durante o treinamento. Um valor de 16 indica que as atualizações de pesos serão escaladas por este fator. Esse parâmetro é importante porque permite um controle fino sobre a magnitude das atualizações dos pesos, o que pode afetar a rapidez e a eficácia da adaptação do modelo.\n",
        "\n",
        "**lora_dropout**: Este parâmetro representa a taxa de \"dropout\" aplicada durante a adaptação do modelo. O valor 0.1 significa que 10% das unidades serão aleatoriamente descartadas (ou \"desligadas\") durante o treinamento. O dropout é uma técnica comum para evitar o overfitting em redes neurais, garantindo que o modelo não se torne excessivamente dependente de qualquer parte específica dos dados de treinamento."
      ],
      "metadata": {
        "id": "uybfqsE7-0jZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4iDCe5R1OOj"
      },
      "outputs": [],
      "source": [
        "# Parâmetros bitsandbytes (QLoRa)\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os parâmetros acima são para a biblioteca bitsandbytes, uma ferramenta para otimização de treinamento de modelos de aprendizado de máquina, em particular para reduzir o uso de memória e acelerar o treinamento. Aqui está a explicação de cada parâmetro:\n",
        "\n",
        "**use_4bit**: Este parâmetro indica se a quantização de 4 bits deve ser usada ou não. Ao definir True, isso significa que o modelo irá utilizar uma representação de 4 bits para os pesos durante o treinamento. Isso reduz significativamente a quantidade de memória necessária, permitindo treinar modelos maiores ou reduzir os requisitos de hardware.\n",
        "\n",
        "**bnb_4bit_compute_dtype**: Este é o tipo de dado usado para cálculos durante o treinamento quando a quantização de 4 bits está ativa. O valor float16 significa que os cálculos serão feitos usando números de ponto flutuante de 16 bits. Isso é geralmente usado para equilibrar a eficiência computacional e a precisão numérica.\n",
        "\n",
        "**bnb_4bit_quant_type**: Especifica o tipo de quantização a ser usado. O valor nf4 é um tipo específico de quantização desenvolvido pela bitsandbytes, otimizado para eficiência e eficácia em treinamento de modelos de aprendizado profundo. Este tipo de quantização é projetado para manter a precisão do modelo enquanto reduz os requisitos de memória.\n",
        "\n",
        "**use_nested_quant**: Indica se uma técnica de quantização aninhada será usada. False significa que essa técnica não será empregada. A quantização aninhada pode ser usada para reduzir ainda mais o uso de memória, aplicando diferentes níveis de quantização a diferentes partes do modelo, mas pode ser mais complexa de implementar e gerenciar.\n",
        "\n",
        "Esses parâmetros são usados para configurar como o modelo de aprendizado profundo irá lidar com a representação e cálculo dos pesos durante o treinamento, visando otimizar o uso de memória e acelerar o processo de treinamento."
      ],
      "metadata": {
        "id": "jCqGrqMz_oZ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhvC2CvF1OOj"
      },
      "outputs": [],
      "source": [
        "# Parâmetros do ajuste fino\n",
        "output_dir = \"saida\"\n",
        "num_train_epochs = 1\n",
        "fp16 = True\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"cosine\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os parâmetros acima são usados para configurar o processo de ajuste fino (fine-tuning) de modelos de aprendizado de máquina, especialmente modelos de linguagem natural. Vamos descrever cada um deles:\n",
        "\n",
        "**output_dir**: Especifica o diretório onde os resultados do treinamento serão salvos.\n",
        "\n",
        "**num_train_epochs**: O número de épocas de treinamento. O valor 1 significa que o modelo passará uma vez por todo o conjunto de dados de treinamento.\n",
        "\n",
        "**fp16**: Indica se deve ser usado o treinamento com precisão mista de ponto flutuante de 16 bits (FP16). O valor True significa que sim, o que pode acelerar o treinamento e reduzir o uso de memória, mantendo uma precisão aceitável.\n",
        "\n",
        "**bf16**: Semelhante ao fp16, mas para o formato bfloat16. False significa que não será utilizado. O formato bfloat16 é outra forma de reduzir o uso de memória e acelerar o treinamento, com impactos ligeiramente diferentes na precisão. Podemos usar fp16 ou bf16, mas não podemos usar ambos simultaneamente.\n",
        "\n",
        "**per_device_train_batch_size**: Tamanho do lote de treinamento por dispositivo. O valor 4 indica que cada dispositivo de treinamento (como uma GPU) processará 4 exemplos por lote.\n",
        "\n",
        "**per_device_eval_batch_size**: Tamanho do lote de avaliação por dispositivo, também definido como 4.\n",
        "\n",
        "**gradient_accumulation_steps**: Número de passos para acumulação de gradientes antes de realizar uma atualização de parâmetros. O valor 1 significa que não há acumulação (cada passo resulta em uma atualização).\n",
        "\n",
        "**gradient_checkpointing**: Habilita o checkpointing de gradientes, que é uma técnica para reduzir o uso de memória ao custo de um tempo de treinamento ligeiramente maior. True indica que está habilitado.\n",
        "\n",
        "**max_grad_norm**: Norma máxima para o corte de gradientes. O valor 0.3 é um valor que ajuda a evitar o problema de explosão de gradientes em treinamentos.\n",
        "\n",
        "**learning_rate**: Taxa de aprendizado inicial. O valor 2e-4 é um valor comum para ajuste fino, proporcionando um equilíbrio entre a velocidade de aprendizado e a estabilidade.\n",
        "\n",
        "**weight_decay**: Taxa de decaimento de peso, usada para regularização. O valor 0.001 é um valor que ajuda a prevenir o overfitting.\n",
        "\n",
        "**optim**: O otimizador usado. \"paged_adamw_32bit\" é uma variante do AdamW otimizado para eficiência em termos de memória.\n",
        "\n",
        "**lr_scheduler_type**: Tipo de agendador de taxa de aprendizado. O valor \"cosine\" indica o uso do agendador cosseno, que ajusta a taxa de aprendizado seguindo uma curva cosseno.\n",
        "\n",
        "**max_steps**: Número máximo de passos de treinamento. O valor -1 significa que o treinamento continuará até que o número de épocas seja alcançado.\n",
        "\n",
        "**warmup_ratio**: Proporção do número total de passos de treinamento usados para o aquecimento linear da taxa de aprendizado. O valor 0.03 significa que 3% do treinamento inicial será usado para aumentar gradualmente a taxa de aprendizado.\n",
        "\n",
        "Esses parâmetros são essenciais para configurar de maneira eficiente o processo de ajuste fino, impactando diretamente na qualidade do modelo treinado, no tempo de treinamento e no uso de recursos computacionais."
      ],
      "metadata": {
        "id": "1Y-OD5X9BLom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y-YqQIO1OOj"
      },
      "outputs": [],
      "source": [
        "# Agrupando sequências em lotes de mesmo comprimento\n",
        "group_by_length = True\n",
        "save_steps = 0\n",
        "logging_steps = 400"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
        "Os parâmetros acima são usados para configurar certos aspectos do processo de treinamento de modelos de aprendizado de máquina, especialmente modelos de linguagem. Eles estão relacionados a como as sequências são agrupadas em lotes e como o progresso do treinamento é registrado e salvo. Vamos detalhar cada um:\n",
        "\n",
        "**group_by_length**: Este parâmetro indica se as sequências devem ser agrupadas por comprimento ao formar lotes de treinamento. Quando True, isso significa que o treinamento agrupará sequências de comprimentos semelhantes juntas em um lote. Esta é uma prática eficiente porque reduz a quantidade de preenchimento (padding) necessário. O preenchimento é usado para garantir que todas as sequências em um lote tenham o mesmo comprimento, mas pode ser um desperdício de recursos computacionais. Agrupar sequências de comprimentos semelhantes minimiza esse desperdício.\n",
        "\n",
        "**save_steps**: Especifica a frequência com que o modelo treinado deve ser salvo. Um valor de 0 indica que o modelo não será salvo automaticamente com base em um número de passos. Em vez disso, o modelo pode ser salvo no final de cada época de treinamento ou manualmente.\n",
        "\n",
        "**logging_steps**: Define a frequência com que as informações de log devem ser registradas. O valor 400 significa que o processo de treinamento registrará informações como a perda de treinamento (loss), métricas de avaliação, entre outros, a cada 400 passos de treinamento. Isso é útil para monitorar o progresso do treinamento e para o ajuste fino dos hiperparâmetros."
      ],
      "metadata": {
        "id": "QZBqDmeXC2Ye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5-jclzon8nz"
      },
      "outputs": [],
      "source": [
        "# Precisão dos dados para treinamento\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O código acima está relacionado à configuração do tipo de dado (dtype) para computação durante o treinamento de modelos de rede neural usando a biblioteca PyTorch. Vamos analisar cada parte do código:\n",
        "\n",
        "**torch**: É uma referência à biblioteca PyTorch, uma biblioteca para aprendizado de máquina e redes neurais.\n",
        "\n",
        "**bnb_4bit_compute_dtype**: Esta é uma variável que armazena uma string representando o tipo de dado desejado para computação. O parâmetro bnb_4bit_compute_dtype foi definido como \"float16\", indicando que a computação deve ser realizada usando números de ponto flutuante de 16 bits.\n",
        "\n",
        "**getattr**: É uma função Python built-in usada para obter um atributo de um objeto. Neste caso, ela está sendo usada para obter um atributo da biblioteca PyTorch com base no valor da string armazenada em bnb_4bit_compute_dtype.\n",
        "\n",
        "O que acontece aqui é que getattr(torch, bnb_4bit_compute_dtype) recupera o tipo de dados de ponto flutuante de 16 bits (torch.float16) da biblioteca PyTorch, com base no valor de bnb_4bit_compute_dtype. Em seguida, esse tipo de dados é atribuído à variável compute_dtype.\n",
        "\n",
        "O uso de compute_dtype no treinamento de modelos de rede neural tem implicações importantes:\n",
        "\n",
        "**Eficiência de Memória**: Usar float16 ao invés de tipos de dados mais comuns como float32 pode reduzir significativamente o uso de memória, permitindo o treinamento de modelos maiores ou a execução de mais processos em paralelo.\n",
        "\n",
        "**Velocidade de Computação**: Muitas GPUs modernas têm otimizações para cálculos float16, o que pode acelerar o treinamento.\n",
        "\n",
        "**Precisão**: Embora float16 possa ser menos preciso do que float32, muitas vezes é suficientemente preciso para tarefas de treinamento de modelos de rede neural."
      ],
      "metadata": {
        "id": "KORbxhs_DgFg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jetJVp9n8rN"
      },
      "outputs": [],
      "source": [
        "# Definindo os parâmetros da quantização\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit = use_4bit,\n",
        "                                bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
        "                                bnb_4bit_compute_dtype = compute_dtype,\n",
        "                                bnb_4bit_use_double_quant = use_nested_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-n7bDY5n8w5"
      },
      "outputs": [],
      "source": [
        "# Carregando o modelo base pré-treinado\n",
        "modelo = AutoModelForCausalLM.from_pretrained(repositorio_hf,\n",
        "                                              quantization_config = bnb_config,\n",
        "                                              device_map = \"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1x315Frn8zg"
      },
      "outputs": [],
      "source": [
        "# Não usaremos o cache\n",
        "modelo.config.use_cache = False\n",
        "modelo.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiFQiy4HpxH_"
      },
      "outputs": [],
      "source": [
        "# Carregando o tokenizador do modelo base\n",
        "tokenizador = AutoTokenizer.from_pretrained(repositorio_hf, trust_remote_code = True)\n",
        "tokenizador.pad_token = tokenizador.eos_token\n",
        "tokenizador.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCTzpQP1pxKz"
      },
      "outputs": [],
      "source": [
        "# Carregando a configuração LoRA\n",
        "peft_config = LoraConfig(lora_alpha = lora_alpha,\n",
        "                         lora_dropout = lora_dropout,\n",
        "                         r = lora_r,\n",
        "                         bias = \"none\",\n",
        "                         task_type = \"CAUSAL_LM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py"
      ],
      "metadata": {
        "id": "-CcONYM_S-cE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gIMSx1KpxN6"
      },
      "outputs": [],
      "source": [
        "# Definindo os parâmetros de treino\n",
        "training_arguments = TrainingArguments(output_dir = output_dir,\n",
        "                                       num_train_epochs = num_train_epochs,\n",
        "                                       per_device_train_batch_size = per_device_train_batch_size,\n",
        "                                       gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "                                       optim = optim,\n",
        "                                       save_steps = save_steps,\n",
        "                                       logging_steps = logging_steps,\n",
        "                                       learning_rate = learning_rate,\n",
        "                                       weight_decay = weight_decay,\n",
        "                                       fp16 = fp16,\n",
        "                                       bf16 = bf16,\n",
        "                                       max_grad_norm = max_grad_norm,\n",
        "                                       max_steps = max_steps,\n",
        "                                       warmup_ratio = warmup_ratio,\n",
        "                                       group_by_length = group_by_length,\n",
        "                                       lr_scheduler_type = lr_scheduler_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja a descrição de Supervised Fine-Tuning e RLHF no videobook."
      ],
      "metadata": {
        "id": "vAQ-sUc_g1Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo os Parâmetros do Fine-Tuning Supervisionado\n",
        "sft_config = SFTConfig(per_device_train_batch_size = 2,\n",
        "                       gradient_accumulation_steps = 4,\n",
        "                       warmup_steps = 5,\n",
        "                       num_train_epochs = 1,\n",
        "                       learning_rate = 2e-4,\n",
        "                       fp16 = True,\n",
        "                       logging_steps = 1,\n",
        "                       optim = \"adamw_8bit\",\n",
        "                       weight_decay = 0.01,\n",
        "                       lr_scheduler_type = \"linear\",\n",
        "                       seed = 3407,\n",
        "                       output_dir = \"outputs\",\n",
        "                       report_to = \"none\",\n",
        "                       max_seq_length = 2048,\n",
        "                       dataset_text_field = \"train\",\n",
        "                       packing = False)"
      ],
      "metadata": {
        "id": "R-oEwLJdpnUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1bKw7kWpxTY"
      },
      "outputs": [],
      "source": [
        "# Definindo os Parâmetros do Fine-Tuning Supervisionado (requer os parâmetros gerais no sft_config acima)\n",
        "dsa_trainer = SFTTrainer(model = modelo,\n",
        "                         train_dataset = dataset_carregado['train'],\n",
        "                         peft_config = peft_config,\n",
        "                         tokenizer = tokenizador,\n",
        "                         args = sft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/trl/en/sft_trainer"
      ],
      "metadata": {
        "id": "JXLuJ_A9VLbb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fBqN7AJtTQ"
      },
      "source": [
        "> Treinamento do Modelo com o Ajuste Fino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LFznML2p6wo"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "dsa_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJXpOgBFuSrc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Salvando o modelo treinado\n",
        "dsa_trainer.model.save_pretrained(modelo_dsa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_4kdK7JdsC"
      },
      "source": [
        "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frlSLPin4IJ4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Novo texto de entrada\n",
        "prompt = \"It's rare that a movie lives up to its hype, even rarer that the hype is transcended by the actual achievement\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/docs/transformers/en/main_classes/pipelines"
      ],
      "metadata": {
        "id": "2ILPVI4NPIz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jdt2kBWo2vJi"
      },
      "outputs": [],
      "source": [
        "# Pipeline de Análise de Sentimentos com o Modelo Ajustado\n",
        "pipe = pipeline(task = \"text-generation\",\n",
        "                model = modelo,\n",
        "                tokenizer = tokenizador,\n",
        "                max_length = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3hDsyOA2vOg"
      },
      "outputs": [],
      "source": [
        "# Executa o pipeline e extrai o resultado\n",
        "resultado = pipe(f\"<s>[INST] {prompt} [/INST]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aZi4M_x2vUu"
      },
      "outputs": [],
      "source": [
        "print(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resultado[0]['generated_text'])"
      ],
      "metadata": {
        "id": "SfeM508hr-q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkQCviG0Zta-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Libera a memória\n",
        "del modelo\n",
        "del pipe\n",
        "del dsa_trainer\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px08AD1dG8Vl"
      },
      "outputs": [],
      "source": [
        "# Carrega o modelo em fp16 e faz o merge com os pesos LoRA\n",
        "base_model = AutoModelForCausalLM.from_pretrained(repositorio_hf,\n",
        "                                                  low_cpu_mem_usage = True,\n",
        "                                                  return_dict = True,\n",
        "                                                  torch_dtype = torch.float16,\n",
        "                                                  device_map = \"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6zPp5ONG8Yp"
      },
      "outputs": [],
      "source": [
        "# Cria o modelo final\n",
        "modelo_dsa_final = PeftModel.from_pretrained(base_model, modelo_dsa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se1-AzXBG_w1"
      },
      "outputs": [],
      "source": [
        "# Faz o merge e descarrega o modelo\n",
        "modelo_dsa_final = modelo_dsa_final.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQn30cRtAZ-P",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Carrega o tokenizador\n",
        "tokenizador_dsa = AutoTokenizer.from_pretrained(repositorio_hf, trust_remote_code = True)\n",
        "tokenizador_dsa.pad_token = tokenizador_dsa.eos_token\n",
        "tokenizador_dsa.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U6l10AxmymR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Salva modelo e tokenizador\n",
        "modelo_dsa_final.save_pretrained('novo_modelo-dsa-llm-projeto2')\n",
        "tokenizador_dsa.save_pretrained('novo_modelo-dsa-llm-projeto2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qtj5FBQFqgb"
      },
      "outputs": [],
      "source": [
        "# Novo texto de entrada\n",
        "prompt = \"It's rare that a movie lives up to its hype, even rarer that the hype is transcended by the actual achievement\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD7Omii_Fqjd"
      },
      "outputs": [],
      "source": [
        "# Cria o pipeline\n",
        "pipe = pipeline(task = \"text-generation\",\n",
        "                model = modelo_dsa_final,\n",
        "                tokenizer = tokenizador_dsa,\n",
        "                max_length = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwVzL7jrFqml"
      },
      "outputs": [],
      "source": [
        "# Executa o pipeline e extrai o resultado\n",
        "resultado = pipe(f\"<s>[INST] {prompt} [/INST]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YWrAyoE07Lh"
      },
      "outputs": [],
      "source": [
        "print(resultado)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos não apenas classificar o sentimento.\n",
        "# Vamos gerar texto positivo e/ou negativo a partir da avaliação (texto) inicial.\n",
        "print(resultado[0]['generated_text'])"
      ],
      "metadata": {
        "id": "yMbswQgMOHAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJq2oQL8Q1C1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Libera a memória da GPU\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c01jjsCwmymS"
      },
      "outputs": [],
      "source": [
        "%watermark -a \"Data Science Academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW_3C2hS1OOt"
      },
      "outputs": [],
      "source": [
        "#%watermark -v -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPpHlQzH1OOt"
      },
      "outputs": [],
      "source": [
        "#%watermark --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lms4A2Yb1OOt"
      },
      "source": [
        "# Fim"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}